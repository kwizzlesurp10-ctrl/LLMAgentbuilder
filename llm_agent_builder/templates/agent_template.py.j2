import anthropic
import os
from typing import Optional, List, Dict, Any
{% if db_path %}
import sqlite3
{% endif %}
{% if enable_multi_step or tools or docs_path %}
import json
{% endif %}
{% if docs_path %}
import chromadb
from chromadb.utils import embedding_functions
import uuid
{% endif %}

class {{ agent_name }}:
    def __init__(self, api_key):
        {% if provider == 'huggingface' %}
        from huggingface_hub import InferenceClient
        self.client = InferenceClient(token=api_key)
        self.model = "{{ model }}"
        {% else %}
        self.client = anthropic.Anthropic(api_key=api_key)
        {% endif %}
        self.prompt = "{{- prompt -}}"
        {% if tools %}
        self.tools = {{ tools | tojson }}
        {% endif %}
        {% if db_path %}
        self.db_path = "{{ db_path }}"
        self.prompt += "\n\nYou have access to a SQLite database. You can query it using the 'query_database' tool."
        # Add database tool definition
        db_tool = {
            "name": "query_database",
            "description": "Execute a SQL query against the SQLite database. Use this to retrieve information.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The SQL query to execute"
                    }
                },
                "required": ["query"]
            }
        }
        if not hasattr(self, 'tools'):
            self.tools = []
        self.tools.append(db_tool)
        {% endif %}

        {% if docs_path %}
        self.docs_path = "{{ docs_path }}"
        self.prompt += "\n\nYou have access to a knowledge base. You can use the 'retrieve_knowledge' tool to search for information."
        self._init_knowledge_base()
        
        rag_tool = {
            "name": "retrieve_knowledge",
            "description": "Search the knowledge base for relevant information.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query to find relevant documents"
                    }
                },
                "required": ["query"]
            }
        }
        if not hasattr(self, 'tools'):
            self.tools = []
        self.tools.append(rag_tool)
        {% endif %}

        {% if tool_implementations %}
        # Injected standard tool implementations
        {% for impl in tool_implementations %}
{{ impl }}
        {% endfor %}
        {% endif %}

    {% if tools or docs_path or db_path %}
    def _execute_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a tool call. Override this method to implement custom tool logic."""
        {% if db_path %}
        if tool_name == "query_database":
            return self.query_database(tool_input["query"])
        {% endif %}
        {% if docs_path %}
        if tool_name == "retrieve_knowledge":
            return self.retrieve_knowledge(tool_input["query"])
        {% endif %}
        # Default implementation - check if method exists on self (for standard tools)
        if hasattr(self, tool_name):
            try:
                method = getattr(self, tool_name)
                # Inspect method signature to blindly pass args or check? 
                # For simplicity, we assume flattened args matching schema properties
                # But safer to pass dict if method expects it?
                # The standard tools defined in library expect distinct arguments (e.g. expression, text).
                # We can unpack tool_input.
                return method(**tool_input)
            except Exception as e:
                return {"error": str(e)}

        # Fallback
        return {"result": f"Tool {tool_name} executed with input: {tool_input}"}
    {% endif %}

    {% if docs_path %}
    def _init_knowledge_base(self):
        """Initialize ChromaDB and ingest documents."""
        print(f"Initializing knowledge base from {self.docs_path}...")
        self.chroma_client = chromadb.Client()
        # Use a simple embedding function (or default)
        self.embedding_fn = embedding_functions.DefaultEmbeddingFunction()
        
        self.collection = self.chroma_client.get_or_create_collection(
            name="knowledge_base",
            embedding_function=self.embedding_fn
        )

        # Basic ingestion (only if empty to avoid re-ingesting on every run in this simple script)
        if self.collection.count() == 0:
            if os.path.exists(self.docs_path):
                for filename in os.listdir(self.docs_path):
                    if filename.endswith(".txt") or filename.endswith(".md"):
                        filepath = os.path.join(self.docs_path, filename)
                        with open(filepath, "r", encoding="utf-8") as f:
                            content = f.read()
                            self.collection.add(
                                documents=[content],
                                metadatas=[{"source": filename}],
                                ids=[str(uuid.uuid4())]
                            )
                print(f"Ingested {self.collection.count()} documents.")
            else:
                print(f"Warning: Docs path {self.docs_path} does not exist.")

    def retrieve_knowledge(self, query: str, n_results: int = 3) -> Dict[str, Any]:
        """Search the knowledge base."""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        return {
            "status": "success",
            "documents": results['documents'][0],
            "metadatas": results['metadatas'][0]
        }
    {% endif %}

    {% if db_path %}
    def query_database(self, query: str) -> Dict[str, Any]:
        """Execute a SQL query against the database."""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute(query)
            results = [dict(row) for row in cursor.fetchall()]
            conn.close()
            return {"status": "success", "results": results}
        except Exception as e:
            return {"status": "error", "message": str(e)}
    {% endif %}

    {% if enable_multi_step %}
    def run_multi_step(self, task: str, max_steps: int = 5) -> str:
        """Run a multi-step workflow where the agent can iterate on the task."""
        messages = [{"role": "user", "content": task}]
        final_result = None
        
        for step in range(max_steps):
            response = self.client.messages.create(
                model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                max_tokens=2048,
                system=self.prompt,
                messages=messages{% if tools %},
                tools=self.tools{% endif %}
            )
            
            # Handle tool use if present
            {% if tools %}
            if response.stop_reason == "tool_use":
                tool_uses = [block for block in response.content if block.type == "tool_use"]
                for tool_use in tool_uses:
                    tool_result = self._execute_tool(tool_use.name, tool_use.input)
                    messages.append({
                        "role": "assistant",
                        "content": response.content
                    })
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": tool_use.id,
                            "content": json.dumps(tool_result)
                        }]
                    })
                continue
            {% endif %}
            
            # Extract text response
            text_content = [block.text for block in response.content if block.type == "text"]
            if text_content:
                final_result = text_content[0]
                messages.append({
                    "role": "assistant",
                    "content": response.content
                })
                
                # Check if task is complete (simple heuristic - can be enhanced)
                if "complete" in final_result.lower() or "finished" in final_result.lower():
                    break
            
            # Add continuation prompt for next step
            if step < max_steps - 1:
                messages.append({
                    "role": "user",
                    "content": "Continue or refine your response if needed."
                })
        
        return final_result or "Multi-step workflow completed."
    {% endif %}

    def run(self, task: str{% if enable_multi_step %}, use_multi_step: bool = False{% endif %}):
        {% if enable_multi_step %}
        if use_multi_step:
            return self.run_multi_step(task)
        {% endif %}
        
        {% if provider == 'huggingface' %}
        messages = [{"role": "system", "content": self.prompt}, {"role": "user", "content": task}]
        response = self.client.chat_completion(
            model=self.model,
            messages=messages,
            max_tokens=2048,
            stream=False
        )
        # Note: Tool calling with HF InferenceClient is model-dependent and may require different handling.
        # For this implementation, we'll focus on text generation.
        return response.choices[0].message.content
        {% else %}
        {% if stream %}
        with self.client.messages.stream(
            model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
            max_tokens=2048,
            system=self.prompt,
            messages=[
                {"role": "user", "content": task}
            ]{% if tools or db_path %},
            tools=self.tools{% endif %}
        ) as stream:
            full_response = ""
            for text in stream.text_stream:
                print(text, end="", flush=True)
                full_response += text
            print()
            
            final_message = stream.get_final_message()
            
            {% if tools or db_path %}
            if final_message.stop_reason == "tool_use":
                # Handle tool use in streaming mode
                tool_uses = [block for block in final_message.content if block.type == "tool_use"]
                tool_results = []
                for tool_use in tool_uses:
                    tool_result = self._execute_tool(tool_use.name, tool_use.input)
                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": tool_use.id,
                        "content": json.dumps(tool_result)
                    })
                
                # Get final response after tool execution (streaming the follow-up)
                with self.client.messages.stream(
                    model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                    max_tokens=2048,
                    system=self.prompt,
                    messages=[
                        {"role": "user", "content": task},
                        {"role": "assistant", "content": final_message.content},
                        {"role": "user", "content": tool_results}
                    ],
                    tools=self.tools
                ) as follow_up_stream:
                    for text in follow_up_stream.text_stream:
                        print(text, end="", flush=True)
                        full_response += text
                    print()
                    return full_response
            {% endif %}
            
            return full_response
        {% else %}
        response = self.client.messages.create(
            model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
            max_tokens=2048,
            system=self.prompt,
            messages=[
                {"role": "user", "content": task}
            ]{% if tools or db_path %},
            tools=self.tools{% endif %}
        )
        
        {% if tools or db_path %}
        # Handle tool use in single-step mode
        if response.stop_reason == "tool_use":
            tool_uses = [block for block in response.content if block.type == "tool_use"]
            tool_results = []
            for tool_use in tool_uses:
                tool_result = self._execute_tool(tool_use.name, tool_use.input)
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_use.id,
                    "content": json.dumps(tool_result)
                })
            
            # Get final response after tool execution
            follow_up = self.client.messages.create(
                model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                max_tokens=2048,
                system=self.prompt,
                messages=[
                    {"role": "user", "content": task},
                    {"role": "assistant", "content": response.content},
                    {"role": "user", "content": tool_results}
                ],
                tools=self.tools
            )
            return follow_up.content[0].text
        {% endif %}
        
        return response.content[0].text
        {% endif %}
        {% endif %}

if __name__ == '__main__':
    import os
    import argparse
    from dotenv import load_dotenv

    load_dotenv()

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the {{ agent_name }} agent.")
    parser.add_argument("--task", default="{{- example_task -}}", help="The task to be performed by the agent")
    args = parser.parse_args()

    # Ensure API key is set
    api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("HUGGINGFACEHUB_API_TOKEN")
    if not api_key:
        raise ValueError("API key not found. Please set ANTHROPIC_API_KEY or HUGGINGFACEHUB_API_TOKEN.")

    try:
        agent = {{ agent_name }}(api_key=api_key)
        print(f"Running {{ agent_name }} with task: {args.task}\n")
        result = agent.run(args.task)
        print("Response:")
        print("-" * 50)
        print(result)
        print("-" * 50)
    except Exception as e:
        print(f"Error running agent: {e}")
