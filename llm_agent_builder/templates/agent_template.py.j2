import os
from typing import Optional, List, Dict, Any
{% if provider == 'google' %}
import google.generativeai as genai
{% endif %}
{% if db_path %}
import sqlite3
{% endif %}
{% if enable_multi_step or tools %}
import json
{% endif %}

class {{ agent_name }}:
    def __init__(self, api_key):
        {% if provider == 'huggingface' %}
        from huggingface_hub import InferenceClient
        self.client = InferenceClient(token=api_key)
        self.model = "{{ model }}"
        {% elif provider == 'google' %}
        genai.configure(api_key=api_key)
        self.model_name = os.environ.get("GOOGLE_GEMINI_MODEL", "{{ model }}")
        self.model = genai.GenerativeModel(self.model_name)
        {% else %}
        import anthropic
        self.client = anthropic.Anthropic(api_key=api_key)
        {% endif %}
        self.prompt = "{{- prompt -}}"
        {% if tools %}
        self.tools = {{ tools | tojson }}
        {% endif %}
        {% if db_path %}
        self.db_path = "{{ db_path }}"
        self.prompt += "\n\nYou have access to a SQLite database. You can query it using the 'query_database' tool."
        # Add database tool definition
        db_tool = {
            "name": "query_database",
            "description": "Execute a SQL query against the SQLite database. Use this to retrieve information.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The SQL query to execute"
                    }
                },
                "required": ["query"]
            }
        }
        if not hasattr(self, 'tools'):
            self.tools = []
        self.tools.append(db_tool)
        {% endif %}

    {% if tools %}
    def _execute_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a tool call. Override this method to implement custom tool logic."""
        {% if db_path %}
        if tool_name == "query_database":
            return self.query_database(tool_input["query"])
        {% endif %}
        # Default implementation - override in subclass for custom tools
        return {"result": f"Tool {tool_name} executed with input: {tool_input}"}
    {% endif %}

    {% if db_path %}
    def query_database(self, query: str) -> Dict[str, Any]:
        """Execute a SQL query against the database."""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute(query)
            results = [dict(row) for row in cursor.fetchall()]
            conn.close()
            return {"status": "success", "results": results}
        except Exception as e:
            return {"status": "error", "message": str(e)}
    {% endif %}

    {% if enable_multi_step %}
    def run_multi_step(self, task: str, max_steps: int = 5) -> str:
        """Run a multi-step workflow where the agent can iterate on the task."""
        {% if provider == 'google' %}
        chat = self.model.start_chat(history=[])
        full_prompt = f"{self.prompt}\n\nUser: {task}"
        final_result = None
        
        for step in range(max_steps):
            response = chat.send_message(full_prompt)
            final_result = response.text
            full_prompt = f"Continue or refine your response if needed."
            
            # Check if task is complete
            if "complete" in final_result.lower() or "finished" in final_result.lower():
                break
        
        return final_result or "Multi-step workflow completed."
        {% else %}
        messages = [{"role": "user", "content": task}]
        final_result = None
        
        for step in range(max_steps):
            response = self.client.messages.create(
                model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                max_tokens=2048,
                system=self.prompt,
                messages=messages{% if tools %},
                tools=self.tools{% endif %}
            )
            
            # Handle tool use if present
            {% if tools %}
            if response.stop_reason == "tool_use":
                tool_uses = [block for block in response.content if block.type == "tool_use"]
                for tool_use in tool_uses:
                    tool_result = self._execute_tool(tool_use.name, tool_use.input)
                    messages.append({
                        "role": "assistant",
                        "content": response.content
                    })
                    messages.append({
                        "role": "user",
                        "content": [{
                            "type": "tool_result",
                            "tool_use_id": tool_use.id,
                            "content": json.dumps(tool_result)
                        }]
                    })
                continue
            {% endif %}
            
            # Extract text response
            text_content = [block.text for block in response.content if block.type == "text"]
            if text_content:
                final_result = text_content[0]
                messages.append({
                    "role": "assistant",
                    "content": response.content
                })
                
                # Check if task is complete (simple heuristic - can be enhanced)
                if "complete" in final_result.lower() or "finished" in final_result.lower():
                    break
            
            # Add continuation prompt for next step
            if step < max_steps - 1:
                messages.append({
                    "role": "user",
                    "content": "Continue or refine your response if needed."
                })
        
        return final_result or "Multi-step workflow completed."
        {% endif %}
    {% endif %}

    def run(self, task: str{% if enable_multi_step %}, use_multi_step: bool = False{% endif %}):
        {% if enable_multi_step %}
        if use_multi_step:
            return self.run_multi_step(task)
        {% endif %}
        
        {% if provider == 'huggingface' %}
        messages = [{"role": "system", "content": self.prompt}, {"role": "user", "content": task}]
        response = self.client.chat_completion(
            model=self.model,
            messages=messages,
            max_tokens=2048,
            stream=False
        )
        # Note: Tool calling with HF InferenceClient is model-dependent and may require different handling.
        # For this implementation, we'll focus on text generation.
        return response.choices[0].message.content
        {% elif provider == 'google' %}
        {% if stream %}
        full_prompt = f"{self.prompt}\n\nUser: {task}\n\nAssistant:"
        response = self.model.generate_content(full_prompt, stream=True)
        full_response = ""
        for chunk in response:
            if chunk.text:
                print(chunk.text, end="", flush=True)
                full_response += chunk.text
        print()
        return full_response
        {% else %}
        full_prompt = f"{self.prompt}\n\nUser: {task}\n\nAssistant:"
        response = self.model.generate_content(full_prompt)
        return response.text
        {% endif %}
        {% else %}
        {% if stream %}
        with self.client.messages.stream(
            model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
            max_tokens=2048,
            system=self.prompt,
            messages=[
                {"role": "user", "content": task}
            ]{% if tools or db_path %},
            tools=self.tools{% endif %}
        ) as stream:
            full_response = ""
            for text in stream.text_stream:
                print(text, end="", flush=True)
                full_response += text
            print()
            
            final_message = stream.get_final_message()
            
            {% if tools or db_path %}
            if final_message.stop_reason == "tool_use":
                # Handle tool use in streaming mode
                tool_uses = [block for block in final_message.content if block.type == "tool_use"]
                tool_results = []
                for tool_use in tool_uses:
                    tool_result = self._execute_tool(tool_use.name, tool_use.input)
                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": tool_use.id,
                        "content": json.dumps(tool_result)
                    })
                
                # Get final response after tool execution (streaming the follow-up)
                with self.client.messages.stream(
                    model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                    max_tokens=2048,
                    system=self.prompt,
                    messages=[
                        {"role": "user", "content": task},
                        {"role": "assistant", "content": final_message.content},
                        {"role": "user", "content": tool_results}
                    ],
                    tools=self.tools
                ) as follow_up_stream:
                    for text in follow_up_stream.text_stream:
                        print(text, end="", flush=True)
                        full_response += text
                    print()
                    return full_response
            {% endif %}
            
            return full_response
        {% else %}
        response = self.client.messages.create(
            model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
            max_tokens=2048,
            system=self.prompt,
            messages=[
                {"role": "user", "content": task}
            ]{% if tools or db_path %},
            tools=self.tools{% endif %}
        )
        
        {% if tools or db_path %}
        # Handle tool use in single-step mode
        if response.stop_reason == "tool_use":
            tool_uses = [block for block in response.content if block.type == "tool_use"]
            tool_results = []
            for tool_use in tool_uses:
                tool_result = self._execute_tool(tool_use.name, tool_use.input)
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_use.id,
                    "content": json.dumps(tool_result)
                })
            
            # Get final response after tool execution
            follow_up = self.client.messages.create(
                model=os.environ.get("ANTHROPIC_MODEL", "{{ model }}"),
                max_tokens=2048,
                system=self.prompt,
                messages=[
                    {"role": "user", "content": task},
                    {"role": "assistant", "content": response.content},
                    {"role": "user", "content": tool_results}
                ],
                tools=self.tools
            )
            return follow_up.content[0].text
        {% endif %}
        
        return response.content[0].text
        {% endif %}
        {% endif %}

if __name__ == '__main__':
    import os
    import argparse
    from dotenv import load_dotenv

    load_dotenv()

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the {{ agent_name }} agent.")
    parser.add_argument("--task", default="{{- example_task -}}", help="The task to be performed by the agent")
    args = parser.parse_args()

    # Ensure API key is set
    api_key = os.environ.get("GOOGLE_GEMINI_KEY") or os.environ.get("HUGGINGFACEHUB_API_TOKEN")
    if not api_key:
        raise ValueError("API key not found. Please set GOOGLE_GEMINI_KEY or HUGGINGFACEHUB_API_TOKEN.")

    try:
        agent = {{ agent_name }}(api_key=api_key)
        print(f"Running {{ agent_name }} with task: {args.task}\n")
        result = agent.run(args.task)
        print("Response:")
        print("-" * 50)
        print(result)
        print("-" * 50)
    except Exception as e:
        print(f"Error running agent: {e}")